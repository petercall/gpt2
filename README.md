# Pretraining

In this repository I pretrain a language model built from scratch similar in architecture to GPT2.